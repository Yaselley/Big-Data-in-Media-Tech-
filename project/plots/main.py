# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18A-ubC4Z76PJ25FhuxsWa4FnH0ihBvGW

#I. Twitter Sentiment Analysis Data Preprocessing
"""

import pandas as pd
import numpy as np
import seaborn as sns 
import matplotlib.pyplot as plt 
import re
from nltk.tokenize import WordPunctTokenizer
from pathlib import Path
import bz2

"""## 1. Data Extraction """

def get_data(file_name):
    reviews = bz2.BZ2File(file_name).readlines()
    reviews = [review.decode("utf-8") for review in reviews]
    target = {'1':0, '2':1}

    label = [target[label[9]] for label in reviews]
    reviews = [review[11:] for review in reviews]
    df = pd.DataFrame(data = {"label":label, "reviews": reviews})
    return df

train_df.head(10)

"""## 2. Pre-Processing"""

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

def missing_zero_values_table(df,df_name):
    '''
    Inputs:
        df: pandas table
        df_name: string of the pandas table name
    Output:
        "df_name has columns_nb columns and rows_nb Rows. There are columns_nb columns that have missing values."
    '''
    zero_val = (df == 0.00).astype(int).sum(axis=0)
    mis_val = df.isnull().sum()
    mis_val_percent = 100 * df.isnull().sum() / len(df)
    mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)
    mz_table = mz_table.rename(
    columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})
    mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']
    mz_table['% Total Zero Missing Values'] = 100 * mz_table['Total Zero Missing Values'] / len(df)
    mz_table['Data Type'] = df.dtypes
    mz_table = mz_table[mz_table.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)
    print (df_name + " has " + str(df.shape[1]) + " columns and " + str(df.shape[0]) + " Rows.\n"      
        "There are " + str(mz_table.shape[0]) + " columns that have missing values.")
    return mz_table
    
missing_zero_values_table(train_df,"train_df")
missing_zero_values_table(test_df,"test_df")

"""## 3. Data Cleaning """

## Remove Punctuations:
import string
punctuation_string = string.punctuation
punctuation_string = punctuation_string.translate({ord(i): None for i in '@'})

def remove_punct(text,punctuation_string):
    text  = "".join([char for char in text if char not in punctuation_string])
    text = re.sub('[0-9]+', '', text)
    return text

## Tokenization :
def tokenization(text):
    text = re.split('\W+', text)
    return text

## Word Replacement :
replaced_words = [("hmmyou",""),("sry","sorry"),("inlove","in love"),("thats",""),("wanna",""),
                  ("soo","so"),("inlove","in love"),("amazingwell","amazing well"),
                  ("messagesorry","message sorry"),("½",""),("tomorrowneed","tomorrow need"),
                  ("tomorrowis","tomorrow is"),("amusedtime","amused time"),("weekendor","weekend or"),
                  ("competitionhope","competition hope"),("partypicnic","party picnic"),
                  ("ahmazing","amazing"),("wont","will not"),("didnt","did not"),("dont","do not"),
                  ("lookin","looking"),("u","you"),("youre","you are"),("nite","night"),("isnt","is not"),
                  ("k",""),("is",""),("doesnt","does not"),("l",""),("x",""),("c",""),("ur","your"),
                  ("e",""),("yall","you all"),("he",""),("us",""),("okim","ok i am"),("jealousi","jealous"),
                  ("srry","sorry"),("itll","it will"),("vs",""),("weeknend","weekend"),("w",""),
                  ("yr","year"),("youve","you have"),("havent","have not"),("iï",""),("gonna","going to"),
                  ("gimme","give me"),("ti",""),("ta",""),("thru","through"),("th",""),("imma","i am going to"),
                  ("wasnt","was not"),("arent","are not"), ("bff","best friend forever"),("sometimesdid","sometimes did"),
                  ("waitt","wait"),("bday","birthday"),("toobut","too but"),("showerand","shower and"),
                  ("innit","is not it"),("surgury","surgery"),("soproudofyo","so proud of you"),("p",""),
                  ("couldnt","could not"),("dohforgot","forgot"),("rih","right"),("b",""),("bmovie","movie"),
                  ("pleaseyour","please your"),("tonite","tonight"),("grea","great"),("se",""),("soonso","soon so"),
                  ("gettin","getting"),("blowin","blowing"),("coz","because"),("thanks","thank"),("st",""),("rd",""),
                  ("gtta","have got to"),("gotta","have got to"),("anythingwondering","anything wondering"),
                  ("annoyedy","annoyed"),("p",""),("beatiful","beautiful"),("multitaskin","multitasking"),
                  ("nightmornin","night morning"),("thankyou","thank you"),("iloveyoutwoooo","i love you two"),
                  ("tmwr","tomorrow"),("wordslooks","words looks"),("ima","i am going to"),("liek","like"),("mr",""),
                  ("allnighter","all nighter"),("tho","though"),("ed",""),("fyou",""),("footlong","foot long"),
                  ("placepiggy","place piggy"),("semiflaky","semi flaky"),("gona","going to"),("tmr","tomorrow"),
                  ("ppl","people"),("n",""),("dis","this"),("dun","done"),("houseee","house"),("havee","have"),
                  ("studyingwhew","studying whew"),("awwyoure","aww you are"),("softyi","softy"),
                  ("weddingyou","wedding you"),("hassnt","has not"),("lowerleft","lower left"),("anywayss","anyway"),
                  ("adoarble","adorable"),("blogyeahhhh","blog yeahhhh"),("billsim","bills i am"),("ps",""),
                  ("cheescake","cheesecake"),("morningafternoonnight","morning after noon night"),
                  ("allstudying","all studying"),("ofcoooursee","of course"),("jst","just"),("shes","she is"),
                  ("sonicswhich","sonics which"),("ouchwaited","ouch waited"),("itll","it will"),("orreply","or reply"),
                  ("somethin","something"),("fridayand","friday and"),("outta","out of"),("herenever","here never")
                 ] 

def replace_words(text,replaced_words):
    ind = -1 
    for word in text:
        ind +=1
        for k in range(len(replaced_words)):
            if word == replaced_words[k][0]:
                text[ind] = replaced_words[k][1]
            elif "http" in word:
                text[ind] = ""
            elif "@" in word:
                text[ind] = ""
            elif "www." in word:
                text[ind] = ""
            elif "Â" in word: 
                text[ind] = ""
            elif "Ã" in word: 
                text[ind] = ""
            elif "½" in word:
                text[ind] = ""
    return text

import nltk
nltk.download('stopwords')

## Remove stopwords :
stopword = nltk.corpus.stopwords.words('english')
print("stopword:\n",stopword)
print("\n\n There are some words that we want to keep, for example 'no', 'nor','not'\n")
words_to_keep = ["not","no","nor"]
stopword = [elem for elem in stopword if not elem in words_to_keep]
stopword.extend(["im","theyre","ive","p","alot","er",""]) # Other stopwords to remove
print("stopword:\n",stopword,"\n")

def remove_stopwords(text,stopword):
    text = [word for word in text if word not in stopword]
    return text

## Stemming 
ps = nltk.PorterStemmer()

def stemming(text):
    text = [ps.stem(word) for word in text]
    return text

## Lemmatization
wn = nltk.WordNetLemmatizer()

def lemmatizer(text):
    text = [wn.lemmatize(word) for word in text]
    return text

def process(df,dir) :
  df['review_punct'] = df['reviews'].apply(lambda x: remove_punct(x,punctuation_string))
  print(df.shape)
  df['review_tokenized'] = df['review_punct'].apply(lambda x: tokenization(x.lower()))
  print(df.shape)
  df['review_tokenized'] = df['review_tokenized'].apply(lambda x: replace_words(x,replaced_words))
  print(df.shape)
  df['review_nonstop'] = df['review_tokenized'].apply(lambda x: remove_stopwords(x,stopword))
  print(df.shape)
  df['review_stemmed'] = df['review_nonstop'].apply(lambda x: stemming(x))
  print(df.shape)
  #df['review_lemmatized'] = df['review_nonstop'].apply(lambda x: lemmatizer(x))
  #print(df.head(10))
  df['review_stemmed'] = df['review_stemmed'].apply(lambda x: ' '.join(str(e) for e in x))
  print(df.shape)
  df.to_csv("alldata{}.csv".format(dir))

import nltk
nltk.download('wordnet')

test_df

test_df.head(10)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# plotting
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import matplotlib.gridspec as gridspec
# %matplotlib inline
import seaborn as sns
sns.set_style("whitegrid")

# Don't print the warnings
import warnings
warnings.filterwarnings("ignore")

"""## 4. Target distribution """

train_df = pd.read_csv("/content/drive/MyDrive/alldatatest.csv")
test_df = pd.read_csv("/content/drive/MyDrive/alldatatest.csv")

import matplotlib.patches as mpatches
fig, ax= plt.subplots(figsize =(5,5))

ax = sns.countplot(x='label', data=train_df, palette=['#DC143C',"#32CD32"]);
for p in ax.patches:
    ax.annotate(p.get_height(), (p.get_x(), p.get_height()))

patch1 = mpatches.Patch(color='#DC143C', label='Negative')
patch2 = mpatches.Patch(color="#32CD32", label='Positive')

plt.legend(handles=[patch1, patch2], 
           bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.title("Distribution of the labels")
plt.show()

train_df.head()

"""## 5. Word Cloud"""

from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator

original_column = 'review_punct' ##### COLUMN TO BE PREPROCESSED ("text" or "selected_text") !!!! ####
train_dfS = train_df[:10000]
# Start with one review:
df_positive = train_dfS[train_dfS['label']==1]
df_negative = train_dfS[train_dfS['label']==0]

tweet_all = " ".join(review for review in train_dfS[original_column])
tweet_positive = " ".join(review for review in df_positive[original_column])
tweet_negative = " ".join(review for review in df_negative[original_column])

fig, ax = plt.subplots(3, 1, figsize  = (30,30))
# Create and generate a word cloud image:
wordcloud_aLL = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(tweet_all)
wordcloud_positive = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(tweet_positive)
wordcloud_negative = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(tweet_negative)

# Display the generated image:
ax[0].imshow(wordcloud_aLL, interpolation='bilinear')
ax[0].set_title('All Tweets', fontsize=30, pad=25)
ax[0].axis('off')
ax[1].imshow(wordcloud_positive, interpolation='bilinear')
ax[1].set_title('Tweets under positive Class',fontsize=30, pad=25)
ax[1].axis('off')
ax[2].imshow(wordcloud_negative, interpolation='bilinear')
ax[2].set_title('Tweets under negative Class',fontsize=30, pad=25)
ax[2].axis('off')
plt.show()

## Review Length :
def plot_dist3(df, feature, title):
    '''
    Input:
        df: [Pandas] Dataset
        feature: [String] Column of tweets
    '''
    df['Character_Count'] = df[feature].apply(lambda x: len(str(x)))
    feature = 'Character_Count'
    # Creating a customized chart. and giving in figsize and everything.
    fig = plt.figure(constrained_layout=True, figsize=(18, 8))
    # Creating a grid of 3 cols and 3 rows.
    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)

    # Customizing the histogram grid.
    ax1 = fig.add_subplot(grid[:2, :2])
    # Set the title.
    ax1.set_title('Histogram')
    # plot the histogram.
    sns.distplot(df.loc[:, feature],
                 hist=True,
                 kde=True,
                 ax=ax1,
                 color='#e74c3c')
    ax1.set(ylabel='Frequency')
    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))

    # Customizing the ecdf_plot.
    ax2 = fig.add_subplot(grid[2:, :2])
    # Set the title.
    ax2.set_title('Empirical CDF')
    # Plotting the ecdf_Plot.
    sns.distplot(df.loc[:, feature],
                 ax=ax2,
                 kde_kws={'cumulative': True},
                 hist_kws={'cumulative': True},
                 color='#e74c3c')
    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))
    ax2.set(ylabel='Cumulative Probability')

    # Customizing the Box Plot.
    ax3 = fig.add_subplot(grid[:, 2])
    # Set title.
    ax3.set_title('Box Plot')
    # Plotting the box plot.
    sns.boxplot(y=feature, data=df, ax=ax3, color='#e74c3c')
    ax3.yaxis.set_major_locator(MaxNLocator(nbins=20))

    plt.suptitle(f'{title}', fontsize=24)
    
plot_dist3(train_dfS, "review_punct",'Characters per all Reviews for the column "text"')

def plot_word_number_histogram(textpo, textng,column_name):
    
    """A function for comparing word counts"""

    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)
    sns.distplot(textpo.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')
    sns.distplot(textng.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')

    axes[0].set_xlabel('Word Count')
    axes[0].set_title('positive')
    axes[1].set_xlabel('Word Count')
    axes[1].set_title('negative')
    
    fig.suptitle('Word counts in tweets for the column "{}"'.format(column_name), fontsize=24, va='baseline')
    
    fig.tight_layout()
    
final_column = 'review_punct'
analysed_column = 'label'
    
plot_word_number_histogram(train_dfS[train_dfS[analysed_column] == 1][original_column],
                           train_dfS[train_dfS[analysed_column] == 0][original_column],original_column)

plot_word_number_histogram(train_dfS[train_dfS[analysed_column] == 1][final_column],
                           train_dfS[train_dfS[analysed_column] == 0][final_column],final_column)

from collections import Counter
import plotly.express as px

def top_most_common_words(dataset,column_name,top_nb,sentiment):
    """
    Inputs:
        Dataset: [Pandas]
        Column_name: [String] The name of the column we are interesting with
        sentiment: [String] "all", "positive", "neutral" or "negative"
    """
    dataset['temp_list'] = dataset[column_name].apply(lambda x:str(x).split())
    top = Counter([item for sublist in dataset['temp_list'] for item in sublist])
    temp = pd.DataFrame(top.most_common(top_nb))
    temp.columns = ['Common_words','count']
    fig = px.bar(temp, x="count", y="Common_words",title='Common Words in {} for {} tweets'.format(column_name,sentiment), orientation='h', 
             width=700, height=700,color='Common_words',text='count')
    return fig.show()
    
top_nb=25
final_column = "review_stemmed"
top_most_common_words(train_dfS,final_column,top_nb,"all")

train_df["review_stemmed"].dropna()
train_df = train_df[train_df['review_stemmed'].notna()]

"""#II. Predicting Sentiment on Social Media Data Using Supervised Approaches"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from textblob import TextBlob
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_curve, auc
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
from time import time

"""### 1. Split Data 80%-20%"""

train_df = train_df[:10000]
X_train, X_validation, y_train, y_validation = train_test_split(train_df['review_stemmed'],train_df['label'], test_size = 0.2, random_state = 1234)

X_train

"""### 2. Vectorization : Bag of Words & TF-IDF & Word2Vec"""

## BoW
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
X_train = X_train.dropna()
countV = CountVectorizer() # Bag Of Words
countV.fit_transform(X_train) # Fit the dictionnary

## TF-IDF 
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

tfidfV = TfidfVectorizer()
tfidfV.fit_transform(X_train)

## Word2Vec 
import gensim

tweets = X_train.values

model_w2v = gensim.models.Word2Vec(
            tweets,
            size=200, # desired no. of features/independent variables
            window=8, # context window size
            min_count=2, # Ignores all words with total frequency lower than 2.                                  
            sg = 5, # 1 for skip-gram model
            hs = 0,
            negative = 10, # for negative sampling
            workers= 32, # no.of cores
            seed = 34
) 

model_w2v.train(tweets, total_examples= len(tweets), epochs=20)

def word_vector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0
    for word in tokens:
        try:
            vec += model_w2v.wv[word].reshape((1, size))
            count += 1.
        except KeyError:  # handling the case where the token is not in vocabulary
            continue
    if count != 0:
        vec /= count
    return vec

train_size = len(tweets)
train_arrays = np.zeros((train_size, 200))

def wordLists(data) :
    size = len(data)
    output = np.zeros((size, 200))
    for i in range(size):
        output[i,:] = word_vector(data[i], 200)
        print(output[i,:])
    return output

"""# III. Train of Different Supervised Models"""

from nltk.collections import LazyMap
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import f1_score
from sklearn import svm
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score
import xgboost as xgb

def test_model_BoW(train_df,test_df,y,y_test,models,names) :
    L,S = [[] for i in range(len(names))],[[] for i in range(len(names))]
    L2,S2 = [],[]
    for i in range(len(models)):
        model_bow = Pipeline([('countV_bayes',countV),('bayes_classifier',models[i])])
        model_bow.fit(train_df,y)
        y_pred_train = model_bow.predict(train_df)
        y_pred_test = model_bow.predict(test_df)
        probas_train = model_bow.predict_proba(train_df)
        probas_test = model_bow.predict_proba(test_df)
        accuracy_train = np.mean(y_pred_train == y)
        accuracy_test = np.mean(y_pred_test == y_test)
        f1_train = f1_score(y_pred_train,y, average='macro')
        f1_test = f1_score(y_pred_test,y_test, average='macro')
        L[i].append(accuracy_train)
        S[i].append(accuracy_test)
        L[i].append(f1_train)
        S[i].append(f1_test)
        L2.append(probas_train)
        S2.append(probas_test)

        print('\n\n\n')
        print("#################Start Train and Test BoW with {}#################".format(names[i]))
        print("For training score Using BoW We reached {} as accuracy".format(accuracy_train))
        print("For testing score Using BoW We reached {} as accuracy".format(accuracy_test))
        print("For training score Using BoW We reached {} as f1-score".format(f1_train))
        print("For testing score Using BoW We reached {} as f1-score".format(f1_test))
        print("##################End Train and Test BoW with {}##################".format(names[i]))
        print('\n\n\n')
    
    return L,S,L2,S2
    
    
def test_model_TFIDF(train_df,test_df,y,y_test,models,names) : 
    L,S = [[] for i in range(len(names))],[[] for i in range(len(names))]
    L2,S2 = [],[]
    for i in range(len(models)):
        model_TFIDF = Pipeline([('tfidfv_bayes',tfidfV),('bayes_classifier',models[i])])
        model_TFIDF.fit(train_df,y)
        y_pred_train = model_TFIDF.predict(train_df)
        y_pred_test = model_TFIDF.predict(test_df)
        accuracy_train = np.mean(y_pred_train == y)
        accuracy_test = np.mean(y_pred_test == y_test)
        probas_train = model_TFIDF.predict_proba(train_df)
        probas_test = model_TFIDF.predict_proba(test_df)
        f1_train = f1_score(y_pred_train,y,average='macro')
        f1_test = f1_score(y_pred_test,y_test,average='macro')
        L[i].append(accuracy_train)
        S[i].append(accuracy_test)
        L[i].append(f1_train)
        S[i].append(f1_test)
        print('\n\n\n')
        print("#################Start Train and Test TF-IDF with {}#################".format(names[i]))
        print("For training score Using TFIDF We reached {} as accuracy".format(accuracy_train))
        print("For testing score Using TFIDF We reached {} as accuracy".format(accuracy_test))
        print("For training score Using TFIDF We reached {} as f1-score".format(f1_train))
        print("For testing score Using TFIDF We reached {} as f1-score".format(f1_test))
        print("#################End Train and Test TF-IDF with {}###################".format(names[i]))
        print('\n\n\n')
        L2.append(probas_train)
        S2.append(probas_test)
    
    return L,S,L2,S2

def train_test_model_W2V(model,train_df,test_df_,y,y_test,model_name) : 
    
    model_w2v = model
    model_w2v.fit(train_df,y)
    print("trained Word2vec")
    probas_train = model_w2v.predict_proba(train_df)
    probas_test = model_w2v.predict_proba(test_df_)
    y_pred_train = model_w2v.predict(train_df)
    y_pred_test = model_w2v.predict(test_df_)
    auc_train = roc_auc_score(pd.get_dummies(y).values, probas_train, multi_class="ovo")
    auc_test = roc_auc_score(pd.get_dummies(y_test).values, probas_test, multi_class="ovo")
    accuracy_train = np.mean(y_pred_train == y)
    accuracy_test = np.mean(y_pred_test == y_test)
    f1_train = f1_score(y_pred_train,y,average='macro')
    f1_test = f1_score(y_pred_test,y_test,average='macro')

    print("For training score Using "+ model_name +" we have {} as auc score, {} as accuracy".format(auc_train,accuracy_train))
    print("For testing score Using " + model_name +" we have {} as auc score, {} as accuracy".format(auc_test,accuracy_test))
    print("END##############################")
    return f1_train,f1_test,y_pred_train,y_pred_test

"""## Naive Bayes """

from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB

train_df = X_train
test_df = X_validation
y = y_train
y_test = y_validation 
train_dfW2V = wordLists(X_train.values)

test_dfW2V = wordLists(X_validation.values)

NB1B,NB2B,NB3B,NB4B = test_model_BoW(train_df,test_df,y,y_test,[MultinomialNB()],["Naïve Bayes"])
NB1T,NB2T,NB3T,NB4T = test_model_TFIDF(train_df,test_df,y,y_test,[MultinomialNB()],["Naïve Bayes"])

"""## Random Forest """

models,names = [RandomForestClassifier(n_estimators=2000)],["Random Forest"]
model,model_name = models[0],names[0]
RDF1B,RDF2B,RDF3B,RDF4B = test_model_BoW(train_df,test_df,y,y_test,models,names)
RDF1T,RDF2T,RDF3T,RDF4T = test_model_TFIDF(train_df,test_df,y,y_test,models,names)
RDF1W,RDF2W,RDF3W,RDF4W = train_test_model_W2V(model,train_dfW2V,test_dfW2V,y,y_test,model_name)

"""## Support Vector Machine"""

from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.calibration import CalibratedClassifierCV
C = 1.0  # SVM regularization parameter

svc = svm.SVC(kernel='linear', C=C,probability=True)

models = [svc]
names = ["SVC"]
model = models[0]
model_name = names[0]

SVM1B,SVM2B,SVM3B,SVM4B = test_model_BoW(train_df,test_df,y,y_test,models,names)
SVM1T,SVM2T,SVM3T,SVM4T = test_model_TFIDF(train_df,test_df,y,y_test,models,names)
SVM1W,SVM2W,SVM3W,SVM4W = train_test_model_W2V(model,train_dfW2V,test_dfW2V,y,y_test,model_name)

svcrbf = svm.SVC(kernel='rbf', gamma=0.7, C=C,probability=True)

models = [svcrbf]
names = ["SVM Kernel"]
model = models[0]
model_name = names[0]

SVM1B,SVM2B,SVM3B,SVM4B = test_model_BoW(train_df,test_df,y,y_test,models,names)
SVM1T,SVM2T,SVM3T,SVM4T = test_model_TFIDF(train_df,test_df,y,y_test,models,names)
SVM1W,SVM2W,SVM3W,SVM4W = train_test_model_W2V(model,train_dfW2V,test_dfW2V,y,y_test,model_name)